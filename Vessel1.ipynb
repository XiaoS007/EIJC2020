{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diving into VesselFinder\n",
    "\n",
    "## Prototyping a piece of code - Part1\n",
    "\n",
    "Let's assume that we are interested in collecting the data coming from this web page : \n",
    "\n",
    "https://www.vesselfinder.com/vessels/MOTIVATION-D-IMO-9301108-MMSI-636092241\n",
    "\n",
    "Interesting informations are located, for example, here: \n",
    "\n",
    "\n",
    "[Informations on Motivation D.](https://ibb.co/nrJ8Ypz)\n",
    "\n",
    "The source code of the web page shows us that they are embedded in a table : \n",
    "\n",
    "[Table for the data](https://ibb.co/Gpc0DZH)\n",
    "\n",
    "Our goal is to put these data in a simple csv file.\n",
    "\n",
    "### Installing some modules\n",
    "\n",
    "We will use 2 moduless to scrape the data :  \n",
    "- requests, that basically simulates a web browser\n",
    "- beautifulsoup4, that reads and parses the html code\n",
    "\n",
    "This is done by using \"pip\", to install the required moduless.\n",
    "\n",
    "Please note the the --user option at the end allows you to install these moduless without admin priviledges on your machine... pretty convenient!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4 --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start to write some code\n",
    "\n",
    "First, we import some standard libraries on Python : \n",
    "\n",
    "- csv, to easily handle csv files\n",
    "- time and datetime, to packages to deal with timestamps and scheduling.\n",
    "\n",
    "Best practices to write cool code with Python recommand to import these natural python libs, first.\n",
    "\n",
    "Importing a library is done using... the \"import\" command! ;)\n",
    "#### Importing the libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we import the libraries from the packages we just installed above, using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining some variables\n",
    "We must declare the variable containing the url we want to read, corresponding to our target vessel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.vesselfinder.com/vessels/MOTIVATION-D-IMO-9301108-MMSI-636092241'\n",
    "scrape_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check we did it well by printing it.... using print(url). Not mandatory, but it's sometimes good to do that for debugging purposes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(url, scrape_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try to connect to the website....\n",
    "This is done by using the \"requests\" command get : requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqs = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we suppose that the webpage is imported in the reqs variable.\n",
    "In order to read it, we use beautiful soup to parse the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(reqs.text, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the result of this by using print()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens here????!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that VesselFinder uses a sort of protection to avoid being scraped by robots...\n",
    "We must identify ourselves as a real browser, and not as a Python script.\n",
    "This is done by defining a \"header\", for requests, with a 'user-agent'.\n",
    "We will use the user-agent of Firefox, for instance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " headers = {'user-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:81.0) Gecko/20100101 Firefox/81.0'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try again to get the page and it's content...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqs = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(reqs.text, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the soup!.... That's much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not mandatory but we will save this as an html page... It's always a good idea to keep a trace of what we are doing.\n",
    "\n",
    "#### Backup the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"motivationD_output.html\", \"w\", encoding='utf-8') as file:\n",
    "        file.write(str(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the source code, once again.\n",
    "We can see that all our data are enclosed in <td> tags....\n",
    "BeautifulSoup to the rescue!\n",
    "we can search all the td things in the soup...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = soup.find_all('td')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we have here? Let's print it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract the desired informations, by just indicating its position into brackets...\n",
    "For example,the coordinates are in position 21.... So we can do someting like :\n",
    "coordinates = data[21].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = data[21].get_text()\n",
    "print(coordinates)\n",
    "lat = coordinates.split('/')[0]\n",
    "lon = coordinates.split('/')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for the timestamp of the position (as date_tag).\n",
    "It's a bit more complicated because we have some html code here...\n",
    "But not that difficult as we can use \"soup\" to read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_tag = data[25]\n",
    "date_tag = str(date_tag)\n",
    "date_tag_soup = BeautifulSoup(date_tag, features=\"lxml\")\n",
    "date_tag = date_tag_soup.td['data-title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(date_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert this date to a more friendly format, using datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_tag = date_tag.replace(',', '').strip(' UTC')\n",
    "date_tag = datetime.strptime(date_tag, '%b %d %Y %H:%M')\n",
    "date_collect = date_tag.strftime('%Y-%m-%d')\n",
    "time_collect = date_tag.strftime('%H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(date_collect, time_collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the speed and the heading (same as coordinates...we must split them!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_spd = data[19].get_text()\n",
    "heading = head_spd.split(' / ')[0]\n",
    "speed = head_spd.split(' / ')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heading,speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also scrape the ETA, the port, the draught... Everything...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But for the moment, we write a csv file for all this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('AIS_Track_motivation.csv', 'a', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    writer.writerow([scrape_time, lat, lon, date_collect, time_collect, heading, speed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "At this stage, we know how to scrape the data for one ship.\n",
    "Everytime we launch this script, we collect new data....\n",
    "Every hour should be enough.\n",
    "Let's enhance this a little bit, by adding new vessels... using a loop and google spreadsheets!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
