{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diving into VesselFinder\n",
    "\n",
    "## Enhancing the code - Part2\n",
    "\n",
    "Let's assume that we are interested in collecting the data coming from several web pages (several vessels...).\n",
    "We don't want to use complicated graphical toolkit, nor databases, but still want to share our tool in a friendlly way with our colleagues....\n",
    "\n",
    "We create a google spreadsheet or an ethercalc one, which are both shareable as a csv file.\n",
    "\n",
    "We'll need pandas, a python library that acts like Excel on stero√Øds to read this file, and enclose our previous script in a loop.\n",
    "\n",
    "\n",
    "### Installing some packages\n",
    "\n",
    "We will use 1 more package to scrape the data :  Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the libraries, including pandas.... plus some extra things (random, time, path...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "from random import randint\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The url changes for the spreadsheet one...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_spread = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vR5OEMAcXUby3T1GmrteVb_sQTEUP0rF9QRXlsl6SEL-7y9Ih30kTERgAQe0V4RMdVHn-QSYhcNYlZb/pub?gid=0&single=true&output=csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reuse the previous script as a function in Python. We will then apply this function to all our urls....\n",
    "The only change is that we also scrape the name of the vessel to name our files based on this name...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_vessel(url):\n",
    "    scrape_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:81.0) Gecko/20100101 Firefox/81.0'}\n",
    "    reqs = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(reqs.text, 'lxml')\n",
    "    data = soup.find_all('td')\n",
    "    coordinates = data[21].get_text()\n",
    "    lat = coordinates.split('/')[0]\n",
    "    lon = coordinates.split('/')[1]\n",
    "    date_tag = data[25]\n",
    "    date_tag = str(date_tag)\n",
    "    date_tag_soup = BeautifulSoup(date_tag, features=\"lxml\")\n",
    "    date_tag = date_tag_soup.td['data-title']\n",
    "    date_tag = date_tag.replace(',', '').strip(' UTC')\n",
    "    date_tag = datetime.strptime(date_tag, '%b %d %Y %H:%M')\n",
    "    date_collect = date_tag.strftime('%Y-%m-%d')\n",
    "    time_collect = date_tag.strftime('%H:%M')\n",
    "    head_spd = data[19].get_text()\n",
    "    heading = head_spd.split(' / ')[0]\n",
    "    speed = head_spd.split(' / ')[1]\n",
    "    \n",
    "    \n",
    "    filename_html = os.path.join('%s_AIS.html' % vessel_name)\n",
    "    filename_csv = os.path.join('%s_AIS.csv' % vessel_name)\n",
    "    \n",
    "    with open(filename_html, \"w\", encoding='utf-8') as file:\n",
    "        file.write(str(soup))\n",
    "    with open(filename_csv, 'a', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "        writer.writerow([scrape_time, lat, lon, date_collect, time_collect, heading, speed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Iteration\n",
    "We can now iterate this function threw our csv file using a loop. For each vessel, we will retrieve the same data, automatically. First we read the csv file a put it in a dataframe. Reading a csv file is smoother with Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_list = pd.read_csv(url_spread)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ship_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we read each row in a loop and apply our retrieve_vessel function to the url.\n",
    "*important* : BE NICE TO APIs! Always place a random pause between too requests, or you'll be banned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in ship_list.iterrows():\n",
    "    url = (ship_list['url'][index])\n",
    "    vessel_name = (ship_list['name'][index])\n",
    "    retrieve_vessel(url)\n",
    "    sleep(randint(1, 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
